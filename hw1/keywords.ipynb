{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статьи были взяты с сайта http://special.kremlin.ru/events/president/news. Ключевые слова были указаны под заголовком каждой статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymorphy2.tokenizers import simple_word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphAnalyzer()\n",
    "def normalize_text(text):\n",
    "    lemmas = []\n",
    "    for t in simple_word_tokenize(text):\n",
    "        lemmas.append(m.parse(t)[0].normal_form)\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_keys = []\n",
    "my_keys = []\n",
    "directory = 'C://Users//ASUS//Desktop//автобрея//hw1//keywords'\n",
    "\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            \n",
    "            with open(file, 'r', encoding = 'utf-8') as r:\n",
    "                lines = r.readlines()\n",
    "                \n",
    "                given_norm = []\n",
    "                my_norm = []\n",
    "                \n",
    "                for i in lines[0].split(','):\n",
    "                    given_norm.append(normalize_text(i))\n",
    "                for i in lines[1].split(','):\n",
    "                    my_norm.append(normalize_text(i))\n",
    "                    \n",
    "                given_keys.append(given_norm)\n",
    "                my_keys.append(my_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пересечения ключевых слов\n",
      "\n",
      "text 1: 1\n",
      "text 2: 2\n",
      "text 3: 1\n",
      "text 4: 2\n",
      "text 5: 3\n"
     ]
    }
   ],
   "source": [
    "print('Пересечения ключевых слов' + '\\n')\n",
    "for i in range(5):\n",
    "    given_set = set(given_keys[i])\n",
    "    my_set = set(my_keys[i])\n",
    "    print('text ' + str(i+1) + ': ' + str(len(given_set.intersection(my_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Подгружаем стоп-слова\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TextRank "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала попробуем с gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    text = re.sub(r'[A-Za-z]', '', text)\n",
    "    text = [m.normal_forms(i)[0] for i in text.split() if i not in\n",
    "            stop]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import keywords as kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_kws = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    filename = str(i) + '.txt'\n",
    "    \n",
    "    with open(filename, 'r', encoding = 'utf-8') as r:\n",
    "        text = r.read()\n",
    "        text = ' '.join(clean_text(text))\n",
    "        gensim_kws.append(kw(normalize_text(text), pos_filter=[], scores=True)[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('деловои форум брикс', 0.17694887281538338),\n",
       "  ('начать год', 0.15433564850462436),\n",
       "  ('россия многие', 0.1367563685740837),\n",
       "  ('страна', 0.13316559100865638),\n",
       "  ('государство', 0.125403674155992),\n",
       "  ('связь наш экономика это', 0.11943007939991718),\n",
       "  ('уважаемыи', 0.11593210593825833)],\n",
       " [('сделать', 0.19663128583421613),\n",
       "  ('сегодня', 0.17716243803523324),\n",
       "  ('ближнии восток', 0.17565883773209792),\n",
       "  ('встреча глава', 0.16728398300112274),\n",
       "  ('святеишество наш', 0.1410912187594509),\n",
       "  ('самыи', 0.13713446295328677),\n",
       "  ('хотеть', 0.13690381649612418)],\n",
       " [('это', 0.3527745441509442),\n",
       "  ('год москва', 0.1633502909309378),\n",
       "  ('производство', 0.14024169987641186),\n",
       "  ('два', 0.12432845566346629),\n",
       "  ('ивановскии область', 0.1193391326428287),\n",
       "  ('создать', 0.10492563685012488),\n",
       "  ('программа воссоздание малое город россия впутин', 0.10449826813057499)],\n",
       " [('портовыи', 0.19724195976497286),\n",
       "  ('комплекс', 0.19073828088640882),\n",
       "  ('портовоиндустриальныи парк отэко', 0.17517567827536493),\n",
       "  ('инфраструктура', 0.16967916034346803),\n",
       "  ('также', 0.1633442025716215),\n",
       "  ('запланировать', 0.15807252772876298),\n",
       "  ('средство', 0.15320151924820277)],\n",
       " [('центр', 0.23637805456099553),\n",
       "  ('сентябрь', 0.1983695700755501),\n",
       "  ('около тысяча', 0.1877787137687954),\n",
       "  ('президент киргизия', 0.1830932842209771),\n",
       "  ('россиискии государство', 0.16364918724421165),\n",
       "  ('полигон', 0.15791168346100143),\n",
       "  ('основнои', 0.15766054646888555)]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_kws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь с библиотекой summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "summa_kws = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    filename = str(i) + '.txt'\n",
    "    \n",
    "    with open(filename, 'r', encoding = 'utf-8') as r:\n",
    "        text = r.read()\n",
    "        summa_kws.append(keywords.keywords(normalize_text(text), additional_stopwords=stop, scores=True)[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('наш экономика', 0.16281698481187268),\n",
       "  ('деловой форум брикс', 0.16215588415490703),\n",
       "  ('год темп', 0.15756788917288161),\n",
       "  ('экономический', 0.14926596329170674),\n",
       "  ('россия', 0.14886844530331236),\n",
       "  ('страна', 0.13127923376158268),\n",
       "  ('развитие', 0.1298754807134002)],\n",
       " [('наш', 0.2504094292681494),\n",
       "  ('самый', 0.16854907902946312),\n",
       "  ('ваш', 0.16739014642745928),\n",
       "  ('встреча', 0.16670463502333177),\n",
       "  ('это', 0.16429600329964805),\n",
       "  ('который', 0.14523451614266977),\n",
       "  ('должный сделать', 0.13211780578182486)],\n",
       " [('это', 0.3938784369449436),\n",
       "  ('год', 0.2169150907224778),\n",
       "  ('предприятие', 0.13027575427522525),\n",
       "  ('весь', 0.12540142525555972),\n",
       "  ('производство', 0.12134351266343057),\n",
       "  ('который', 0.12027365119516274),\n",
       "  ('программа воссоздание малое город россия', 0.10775277011909908)],\n",
       " [('отэко', 0.23192687581787788),\n",
       "  ('инфраструктура', 0.19381677198081457),\n",
       "  ('портовый', 0.1913151303382091),\n",
       "  ('средство', 0.18951566949670418),\n",
       "  ('запланировать', 0.17861866883270625),\n",
       "  ('проект', 0.177511469243775),\n",
       "  ('перевалочный комплекс', 0.17135351734591364)],\n",
       " [('около', 0.22626259712547506),\n",
       "  ('центр', 0.21530219776868995),\n",
       "  ('киргизия', 0.20551102334028404),\n",
       "  ('сентябрь', 0.20431315384417717),\n",
       "  ('глава российский', 0.1821589338273784),\n",
       "  ('тысяча', 0.18165580237980325),\n",
       "  ('основной', 0.16098065288955804)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summa_kws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем анализатор списком стоп-слов\n",
    "rake = RAKE.Rake(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_kws = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    filename = str(i) + '.txt'\n",
    "    \n",
    "    with open(filename, 'r', encoding = 'utf-8') as r:\n",
    "        text = r.read()\n",
    "        rake_kws.append(rake.run(normalize_text(text), maxWords=3, minFrequency=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('брикс', 1.875),\n",
       "  ('хотеть', 1.6666666666666667),\n",
       "  ('россия', 1.6666666666666667),\n",
       "  ('партнёр', 1.5),\n",
       "  ('весь', 1.3333333333333333),\n",
       "  ('прежде', 1.3333333333333333),\n",
       "  ('инвестиция', 1.0)],\n",
       " [('ближний восток', 4.333333333333333),\n",
       "  ('наш встреча', 3.761904761904762),\n",
       "  ('это регион', 3.35),\n",
       "  ('весь', 1.75),\n",
       "  ('спросить', 1.6666666666666667),\n",
       "  ('поэтому', 1.6666666666666667),\n",
       "  ('целое', 1.6666666666666667),\n",
       "  ('знать', 1.6666666666666667),\n",
       "  ('это', 1.6),\n",
       "  ('встреча', 1.4285714285714286),\n",
       "  ('мочь', 1.4),\n",
       "  ('хотеть', 1.3333333333333333),\n",
       "  ('сегодня', 1.3333333333333333),\n",
       "  ('происходить', 1.0),\n",
       "  ('известно', 1.0),\n",
       "  ('борьба', 1.0),\n",
       "  ('несмотря', 1.0),\n",
       "  ('беспокоить', 1.0)],\n",
       " [('прекрасный русский город', 8.3),\n",
       "  ('это год', 4.4),\n",
       "  ('речь идти', 4.0),\n",
       "  ('владимир владимир', 4.0),\n",
       "  ('во-первых', 4.0),\n",
       "  ('очень важный', 4.0),\n",
       "  ('ивановский область', 3.95),\n",
       "  ('город иваново', 3.8),\n",
       "  ('мочь сказать', 3.666666666666667),\n",
       "  ('это', 2.066666666666667),\n",
       "  ('создать', 2.0),\n",
       "  ('область', 1.75),\n",
       "  ('инвестиция', 1.75),\n",
       "  ('проект', 1.5),\n",
       "  ('иваново', 1.5),\n",
       "  ('грант', 1.3333333333333333),\n",
       "  ('москва', 1.0),\n",
       "  ('кроме', 1.0),\n",
       "  ('путин', 1.0),\n",
       "  ('воскресенский', 1.0),\n",
       "  ('представлять', 1.0),\n",
       "  ('надеяться', 1.0),\n",
       "  ('добраться', 1.0),\n",
       "  ('связать', 1.0),\n",
       "  ('образ', 1.0),\n",
       "  ('прежде', 1.0)],\n",
       " [('ознакомиться', 1.0)],\n",
       " []]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_kws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    text = re.sub(r'[A-Za-z]', '', text)\n",
    "    text = [m.normal_forms(i)[0] for i in text.split() if i not in\n",
    "            stop]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(1, 6):\n",
    "    filename = str(i) + '.txt'\n",
    "    \n",
    "    with open(filename, 'r', encoding = 'utf-8') as r:\n",
    "        text = r.read()\n",
    "        corpus.append(' '.join(clean_text(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(corpus):\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    #матрица с количеством вхождений слов\n",
    "    matrix = X.toarray()\n",
    "    #обратная матрица\n",
    "    rev_matrix = np.transpose(matrix)\n",
    "    words = vectorizer.get_feature_names()\n",
    "    return words, rev_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, rev_matrix = index(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7\n",
    "tfidf_kws = []\n",
    "\n",
    "for doc in corpus:\n",
    "    matrix = tfidf.fit_transform(doc.split())\n",
    "    feature_array = np.array(tfidf.get_feature_names())\n",
    "    \n",
    "    tfidf_sorting = np.argsort(matrix.toarray()).flatten()[::-1]\n",
    "    top_n = feature_array[tfidf_sorting][:n]\n",
    "    tfidf_kws.append(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['спасибо', 'являться', 'мирный', 'макроэкономический',\n",
       "        'максимально', 'малое', 'масштабный'], dtype='<U21'),\n",
       " array(['добрый', 'являться', 'иерарх', 'мой', 'много', 'многий', 'многие'],\n",
       "       dtype='<U16'),\n",
       " array(['выглядеть', 'январь', 'краткий', 'комплексный', 'комфортный',\n",
       "        'конечно', 'конечный'], dtype='<U22'),\n",
       " array(['россия', 'являться', 'инвестор', 'инфраструктура', 'кластер',\n",
       "        'компания', 'комплекс'], dtype='<U21'),\n",
       " array(['жээнбеков', 'этап', 'донгузый', 'около', 'область', 'наступление',\n",
       "        'на'], dtype='<U15')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_kws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Точность, полнота, F-мера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRank: gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохраняю только ключевые слова без числовых значений \n",
    "gensim = []\n",
    "\n",
    "for i in gensim_kws:\n",
    "    arr = []\n",
    "    for j in i:\n",
    "        arr.append(j[0])\n",
    "    gensim.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1: (0.0, 0.0, 0.0, None)\n",
      "text 2: (0.0, 0.0, 0.0, None)\n",
      "text 3: (0.0, 0.0, 0.0, None)\n",
      "text 4: (0.0, 0.0, 0.0, None)\n",
      "text 5: (0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "#Точность, полнота, f-мера и support относительно моих ключевых слов\n",
    "n = 0\n",
    "for i, j in zip(my_keys, gensim):\n",
    "    n += 1\n",
    "    print('text ' + str(n) + ': ' + str(precision_recall_fscore_support(i, j[:len(i)], average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1: (0.0, 0.0, 0.0, None)\n",
      "text 2: (0.0, 0.0, 0.0, None)\n",
      "text 3: (0.0, 0.0, 0.0, None)\n",
      "text 4: (0.0, 0.0, 0.0, None)\n",
      "text 5: (0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "#Точность, полнота, f-мера и support относительно данных ключевых слов\n",
    "n = 0\n",
    "for i, j in zip(given_keys, gensim):\n",
    "    n += 1\n",
    "    print('text ' + str(n) + ': ' + str(precision_recall_fscore_support(i, j[:len(i)], average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRank: summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохраняю только ключевые слова без числовых значений \n",
    "summa = []\n",
    "\n",
    "for i in summa_kws:\n",
    "    arr = []\n",
    "    for j in i:\n",
    "        arr.append(j[0])\n",
    "    summa.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1: (0.0, 0.0, 0.0, None)\n",
      "text 2: (0.0, 0.0, 0.0, None)\n",
      "text 3: (0.0, 0.0, 0.0, None)\n",
      "text 4: (0.0, 0.0, 0.0, None)\n",
      "text 5: (0.16666666666666666, 0.16666666666666666, 0.16666666666666666, None)\n"
     ]
    }
   ],
   "source": [
    "#Точность, полнота, f-мера и support относительно моих ключевых слов\n",
    "n = 0\n",
    "for i, j in zip(my_keys, summa):\n",
    "    n += 1\n",
    "    print('text ' + str(n) + ': ' + str(precision_recall_fscore_support(i, j[:len(i)], average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1: (0.0, 0.0, 0.0, None)\n",
      "text 2: (0.0, 0.0, 0.0, None)\n",
      "text 3: (0.0, 0.0, 0.0, None)\n",
      "text 4: (0.0, 0.0, 0.0, None)\n",
      "text 5: (0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "#Точность, полнота, f-мера и support относительно данных ключевых слов\n",
    "n = 0\n",
    "for i, j in zip(given_keys, summa):\n",
    "    n += 1\n",
    "    print('text ' + str(n) + ': ' + str(precision_recall_fscore_support(i, j[:len(i)], average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохраняю только ключевые слова без числовых значений \n",
    "rake = []\n",
    "\n",
    "for i in rake_kws:\n",
    "    arr = []\n",
    "    for j in i:\n",
    "        arr.append(j[0])\n",
    "    rake.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1: (0.0, 0.0, 0.0, None)\n",
      "text 2: (0.0, 0.0, 0.0, None)\n",
      "text 3: (0.0, 0.0, 0.0, None)\n",
      "text 4: (0.0, 0.0, 0.0, None)\n",
      "text 5: (0.0, 0.0, 0.0, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Точность, полнота, f-мера и support относительно моих ключевых слов\n",
    "n = 0\n",
    "for i, j in zip(my_keys, rake):\n",
    "    n += 1\n",
    "    if len(i) >= len(j):\n",
    "        ii = i[:len(j)]\n",
    "        jj = j\n",
    "    else:\n",
    "        jj = j[:len(i)]\n",
    "        ii = i\n",
    "    print('text ' + str(n) + ': ' + str(precision_recall_fscore_support(ii, jj, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1: (0.25, 0.25, 0.25, None)\n",
      "text 2: (0.0, 0.0, 0.0, None)\n",
      "text 3: (0.0, 0.0, 0.0, None)\n",
      "text 4: (0.0, 0.0, 0.0, None)\n",
      "text 5: (0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "#Точность, полнота, f-мера и support относительно данных ключевых слов\n",
    "n = 0\n",
    "for i, j in zip(given_keys, rake):\n",
    "    n += 1\n",
    "    if len(i) >= len(j):\n",
    "        ii = i[:len(j)]\n",
    "        jj = j\n",
    "    else:\n",
    "        jj = j[:len(i)]\n",
    "        ii = i\n",
    "    print('text ' + str(n) + ': ' + str(precision_recall_fscore_support(ii, jj, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1: (0.0, 0.0, 0.0, None)\n",
      "text 2: (0.0, 0.0, 0.0, None)\n",
      "text 3: (0.0, 0.0, 0.0, None)\n",
      "text 4: (0.0, 0.0, 0.0, None)\n",
      "text 5: (0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "#Точность, полнота, f-мера и support относительно моих ключевых слов\n",
    "n = 0\n",
    "for i, j in zip(my_keys, tfidf_kws):\n",
    "    n += 1\n",
    "    print('text ' + str(n) + ': ' + str(precision_recall_fscore_support(i, j[:len(i)], average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1: (0.0, 0.0, 0.0, None)\n",
      "text 2: (0.0, 0.0, 0.0, None)\n",
      "text 3: (0.0, 0.0, 0.0, None)\n",
      "text 4: (0.0, 0.0, 0.0, None)\n",
      "text 5: (0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "#Точность, полнота, f-мера и support относительно данных ключевых слов\n",
    "n = 0\n",
    "for i, j in zip(given_keys, tfidf_kws):\n",
    "    n += 1\n",
    "    print('text ' + str(n) + ': ' + str(precision_recall_fscore_support(i, j[:len(i)], average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все использованные методы автоматического выделения ключевых слов справились плохо. Ненулевые значения точности, полноты и F-меры (и то лишь для одного текста) присутствуют только в RAKE и TextRank. Какие могут быть тому причины?\n",
    "\n",
    "1. Tf-idf не выделяет n-граммы, это могло стать причиной того, что в результатах этого метода одни нули.\n",
    "2. Возможно, используемые тексты слишком длинные для данных методов.\n",
    "3. Ключевые слова, предложенные на сайте, откуда были взяты тексты, больше похожи на теги. Однако, это не объясняет нулевой корреляции результатов с ключевыми словами, выделенными мной."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
